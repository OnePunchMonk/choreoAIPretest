{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualisation"
      ],
      "metadata": {
        "id": "tOo8jrwsxOQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "def load_dance_dataset(file_path):\n",
        "    try:\n",
        "        data = np.load(file_path)\n",
        "        print(f\"Dataset loaded successfully from {file_path}. Data shape: {data.shape}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "def animate_dance(data, interval=50):\n",
        "    num_joints, num_timesteps, _ = data.shape\n",
        "\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.set_title(\"3D Dance Animation\")\n",
        "\n",
        "\n",
        "    ax.set_xlim(np.min(data[:,:,0]) - 0.1, np.max(data[:,:,0]) + 0.1)\n",
        "    ax.set_ylim(np.min(data[:,:,1]) - 0.1, np.max(data[:,:,1]) + 0.1)\n",
        "    ax.set_zlim(np.min(data[:,:,2]) - 0.1, np.max(data[:,:,2]) + 0.1)\n",
        "    ax.set_xlabel(\"X\")\n",
        "    ax.set_ylabel(\"Y\")\n",
        "    ax.set_zlabel(\"Z\")\n",
        "    scat = ax.scatter([], [], [], c='red', s=50)\n",
        "\n",
        "    def init():\n",
        "        scat._offsets3d = ([], [], [])\n",
        "        return scat,\n",
        "\n",
        "    def update(frame):\n",
        "        x = data[:, frame, 0]\n",
        "        y = data[:, frame, 1]\n",
        "        z = data[:, frame, 2]\n",
        "        scat._offsets3d = (x, y, z)\n",
        "        return scat,\n",
        "\n",
        "    anim = FuncAnimation(fig, update, frames=num_timesteps, init_func=init, interval=interval, blit=False)\n",
        "    plt.close(fig)\n",
        "    return anim\n",
        "\n",
        "file_path = \"/content/mariel_knownbetter.npy\"\n",
        "dance_data = load_dance_dataset(file_path)\n",
        "if dance_data is not None:\n",
        "    anim = animate_dance(dance_data, interval=50)\n",
        "    display(HTML(anim.to_jshtml()))\n"
      ],
      "metadata": {
        "id": "OSf9mAJdroRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "psbe67u1xRey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "\n",
        "def slice_dance_phrases(mocap_data, window_size=30, step_size=10):\n",
        "    _, total_timesteps, _ = mocap_data.shape\n",
        "    phrases = []\n",
        "    for start in range(0, total_timesteps - window_size + 1, step_size):\n",
        "        phrase = mocap_data[:, start:start+window_size, :]\n",
        "        phrases.append(phrase)\n",
        "    return phrases\n",
        "\n",
        "# For synthetic labels, we define a small vocabulary.\n",
        "vocab = {\"spin\": 0, \"jump\": 1, \"kick\": 2, \"step\": 3, \"wave\": 4, \"run\":5}\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "def generate_synthetic_labels(num_phrases):\n",
        "    labels = []\n",
        "    possible_labels = list(vocab.keys())\n",
        "    for _ in range(num_phrases):\n",
        "        labels.append(random.choice(possible_labels))\n",
        "    return labels\n",
        "\n",
        "class DanceTextDataset(Dataset):\n",
        "    def __init__(self, mocap_data, window_size=30, step_size=10):\n",
        "        super().__init__()\n",
        "        self.phrases = slice_dance_phrases(mocap_data, window_size, step_size)\n",
        "        self.labels = generate_synthetic_labels(len(self.phrases))\n",
        "        self.window_size = window_size\n",
        "        self.num_joints = mocap_data.shape[0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.phrases)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        phrase = self.phrases[idx]\n",
        "        phrase = phrase.transpose(1, 0, 2).reshape(self.window_size, -1)\n",
        "        label = self.labels[idx]\n",
        "        token_idx = vocab[label]\n",
        "        token_tensor = torch.tensor([token_idx], dtype=torch.long)\n",
        "        phrase_tensor = torch.tensor(phrase, dtype=torch.float)\n",
        "        return phrase_tensor, token_tensor\n",
        "\n"
      ],
      "metadata": {
        "id": "5DJ0TD5K4HIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Model"
      ],
      "metadata": {
        "id": "yV_2T3VWxWbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DanceEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, output_dim=64, num_layers=1):\n",
        "        super(DanceEncoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, window_size, input_dim)\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        embedding = self.fc(hn[-1])\n",
        "        return embedding  # (batch, output_dim)\n",
        "\n",
        "# Text Encoder: processes tokenized text (for simplicity, each label is a one-word sequence)\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=128, output_dim=64, num_layers=1):\n",
        "        super(TextEncoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Since our text is very short, we simply average embeddings.\n",
        "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_length) where seq_length=1 here.\n",
        "        emb = self.embedding(x)  # (batch, seq_length, embedding_dim)\n",
        "        # For one-word labels, simply squeeze the sequence dimension.\n",
        "        emb = emb.squeeze(1)  # (batch, embedding_dim)\n",
        "        out = self.fc(emb)\n",
        "        return out  # (batch, output_dim)\n",
        "\n"
      ],
      "metadata": {
        "id": "V8nFv5YwUtkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Learning"
      ],
      "metadata": {
        "id": "wiWzR6LNxb5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def contrastive_loss(dance_embeds, text_embeds, temperature=0.07):\n",
        "    \"\"\"\n",
        "    Compute InfoNCE loss between dance and text embeddings.\n",
        "    Assumes embeddings are normalized.\n",
        "    \"\"\"\n",
        "    logits = torch.matmul(dance_embeds, text_embeds.t()) / temperature\n",
        "    batch_size = logits.shape[0]\n",
        "    labels = torch.arange(batch_size).to(logits.device)\n",
        "    loss_dance2text = F.cross_entropy(logits, labels)\n",
        "    loss_text2dance = F.cross_entropy(logits.t(), labels)\n",
        "    return (loss_dance2text + loss_text2dance) / 2\n",
        "\n"
      ],
      "metadata": {
        "id": "RDume33TxfPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "_MjHIuxXxiGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mocap_data=dance_data\n",
        "window_size = 30\n",
        "step_size = 10\n",
        "dataset = DanceTextDataset(mocap_data, window_size=window_size, step_size=step_size)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "input_dim = 55 * 3\n",
        "dance_encoder = DanceEncoder(input_dim=input_dim, hidden_dim=128, output_dim=64)\n",
        "text_encoder = TextEncoder(vocab_size=len(vocab), embedding_dim=50, hidden_dim=128, output_dim=64)\n",
        "optimizer = optim.Adam(list(dance_encoder.parameters()) + list(text_encoder.parameters()), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for dance_phrase, token_tensor in dataloader:\n",
        "        dance_embed = dance_encoder(dance_phrase)  # (batch, 64)\n",
        "        text_embed = text_encoder(token_tensor)      # (batch, 64)\n",
        "        dance_embed = F.normalize(dance_embed, dim=1)\n",
        "        text_embed = F.normalize(text_embed, dim=1)\n",
        "        loss = contrastive_loss(dance_embed, text_embed)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "duVxZvL7xjpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "NJWw9f53xrRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "holdout_size = 100\n",
        "holdout_dataset = torch.utils.data.Subset(dataset, list(range(len(dataset)-holdout_size, len(dataset))))\n",
        "holdout_loader = DataLoader(holdout_dataset, batch_size=holdout_size, shuffle=False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for dance_phrases_hold, token_hold in holdout_loader:\n",
        "        dance_emb_hold = F.normalize(dance_encoder(dance_phrases_hold), dim=1)\n",
        "        text_emb_hold = F.normalize(text_encoder(token_hold), dim=1)\n",
        "        break\n",
        "\n",
        "def retrieve_dance_from_text(query_text, dance_embeds, dance_dataset):\n",
        "    \"\"\"\n",
        "    Given a natural language query (e.g., \"jump\"), retrieve the closest dance phrase from the holdout set.\n",
        "    \"\"\"\n",
        "    # Tokenize the query (assuming query is a single word in our simple vocabulary)\n",
        "    token_idx = vocab.get(query_text, None)\n",
        "    if token_idx is None:\n",
        "        print(\"Unknown query word.\")\n",
        "        return None\n",
        "    query_tensor = torch.tensor([[token_idx]])\n",
        "    with torch.no_grad():\n",
        "        query_embed = F.normalize(text_encoder(query_tensor), dim=1)\n",
        "        # Compute cosine similarities with holdout dance embeddings.\n",
        "        sims = torch.matmul(query_embed, dance_emb_hold.t())\n",
        "        best_idx = sims.argmax(dim=1).item()\n",
        "        # Retrieve the corresponding dance phrase (and its synthetic label).\n",
        "        retrieved_phrase, retrieved_label = dance_dataset[len(dance_dataset)-holdout_size + best_idx]\n",
        "        return retrieved_phrase, retrieved_label\n",
        "\n",
        "# Example 2: Generating natural language from a dance sequence input.\n",
        "def retrieve_text_from_dance(query_dance, text_embeds, text_dataset):\n",
        "    \"\"\"\n",
        "    Given a dance sequence query, retrieve the closest text description from the holdout set.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        query_embed = F.normalize(dance_encoder(query_dance.unsqueeze(0)), dim=1)\n",
        "        sims = torch.matmul(query_embed, text_emb_hold.t())\n",
        "        best_idx = sims.argmax(dim=1).item()\n",
        "        _, retrieved_token = holdout_dataset[best_idx]\n",
        "        retrieved_word = inv_vocab[retrieved_token.item()]\n",
        "        return retrieved_word\n",
        "\n",
        "retrieved_phrase, retrieved_label = retrieve_dance_from_text(\"wave\", dance_emb_hold, holdout_dataset)\n",
        "print(\"For text query 'jump', retrieved dance phrase has synthetic label:\", inv_vocab[retrieved_label.item()])\n",
        "\n",
        "\n",
        "sample_dance, sample_token = holdout_dataset[0]\n",
        "retrieved_text = retrieve_text_from_dance(sample_dance, text_emb_hold, holdout_dataset)\n",
        "print(\"For a dance sequence query, retrieved text description is:\", retrieved_text)\n"
      ],
      "metadata": {
        "id": "L4oOs9eHxosy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}